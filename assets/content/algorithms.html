<h1><i class="fa-solid fa-gears"></i> Algorithms</h1>
<p class="lead"> In this section you can find information about the OOD detection algorithms used in the experiments.</p>
<ul class="lead">
    <li><strong>EBO</strong> (<a href="https://proceedings.neurips.cc/paper/2020/hash/f5496252609c43eb8a3d147ab9b9c006-Abstract.html" target="_blank" rel="noopener noreferrer">Liu, W. et al., 2020</a>): The Energy-based model for OOD detection computes an energy score derived from the log-sum-exp of the model's logits. Unlike softmax confidence, the energy score captures the unnormalized log-likelihood of the input under the model. Lower energy values are typically associated with ID data, while higher energy indicates potential OOD samples.</li>
    <li><strong>fDBD</strong> (<a href="https://arxiv.org/abs/2312.11536" target="_blank" rel="noopener noreferrer">Liu, L., Qin, Y., 2023</a>): The Fast Decision Boundary based OOD Detector (fDBD) is a computationally efficient, post-hoc OOD detection method that estimates the distance of a sample's feature representation to the decision boundary. By deriving a closed-form lower bound for this distance, fDBD identifies that ID samples typically lie farther from the decision boundary compared to OOD samples.</li>
    <li><strong>GEN</strong> (<a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_GEN_Pushing_the_Limits_of_Softmax-Based_Out-of-Distribution_Detection_CVPR_2023_paper.html" target="_blank" rel="noopener noreferrer">Liu, X. et al., 2023</a>): The Generalized Entropy (GEN) method is a post-hoc OOD detection approach that leverages the predictive distribution of a pre-trained softmax classifier. Without requiring access to training data or model re-training, GEN computes an entropy-based score solely from the model's output probabilities.</li>
    <li><strong>KLM</strong> (<a href="https://proceedings.mlr.press/v162/hendrycks22a" target="_blank" rel="noopener noreferrer">Hendrycks, D. et al., 2022</a>): The Kernel Log-Magnitude (KLM) method is a density-based OOD detection technique that estimates the likelihood of a sample in feature space using kernel density estimation. The detection score is computed from the log-magnitude of the estimated density.</li>
    <li><strong>KNN</strong> (<a href="https://proceedings.mlr.press/v162/sun22d" target="_blank" rel="noopener noreferrer">Sun, Y. et al., 2022</a>): The K-Nearest Neighbors (KNN) baseline performs OOD detection by computing distances between a test sample’s feature embedding and those of training data.</li>
    <li><strong>MDS</strong> (<a href="https://proceedings.neurips.cc/paper/2018/hash/abdeb6f575ac5c6676b747bca8d09cc2-Abstract.html" target="_blank" rel="noopener noreferrer">Lee, K. et al., 2018</a>): The Mahalanobis Distance-based Score (MDS) method models class-conditional feature distributions. It computes the Mahalanobis distance of a test sample to the nearest class mean in feature space.</li>
    <li><strong>NNGuide</strong> (<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Park_Nearest_Neighbor_Guidance_for_Out-of-Distribution_Detection_ICCV_2023_paper.html" target="_blank" rel="noopener noreferrer">Park, J. et al., 2023</a>): Nearest Neighbor Guidance (NNGuide) is a post-hoc OOD detection method that adjusts classifier confidence scores based on the similarity between test inputs and their nearest neighbors in the training set. By guiding the classifier's output to respect the boundary geometry of the data manifold, NNGuide reduces overconfidence in regions far from the training data, thereby enhancing the detection of OOD samples.</li>
    <li><strong>Relation</strong> (<a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/886ed40d7882c9f891824e42a452c228-Abstract-Conference.html" target="_blank" rel="noopener noreferrer">Kim, J.H. et al., 2023</a>): The Relation method constructs a Neural Relation Graph to model sample-wise relational structures in the feature space. By learning the similarity relationships among samples, it captures contextual dependencies and distinguishes outlier or mislabeled data based on inconsistencies within this relational graph. The framework unifies the identification of label noise and OOD instances, enabling robust detection in scenarios with complex data irregularities.</li>
  
</ul>
<p class="muted">Use this section to review the algorithms’ rationale and settings before comparing results across tables.</p>